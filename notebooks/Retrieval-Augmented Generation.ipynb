{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b19d95-3069-434a-bcb0-3166918bf8c6",
   "metadata": {},
   "source": [
    "##### Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5188fba7-484c-4457-8d47-fba2629bea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102334e4-a16f-4e54-897a-05f8d893820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "#os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd0862e6-6b9e-4abb-863d-0b130746f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../open_router_api_key.txt\", \"r\") as fi:\n",
    "    api_key = fi.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce45c5-f84e-48b5-86ef-0fe60d5d61ea",
   "metadata": {},
   "source": [
    "In this exercise, you'll put together a RAG system and compare outputs from RAG vs. just querying an LLM.\n",
    "\n",
    "For this exercise, you'll be asking about Subspace-Constrained LoRA (SC-LoRA), a new technique described in [a recent article publised on arXiv.org](https://arxiv.org/abs/2505.23724). You've been provided the text of this article in the file 2505.23724v1.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b2740-5dc3-4808-a24e-5610525ee7bb",
   "metadata": {},
   "source": [
    "### Part 1: Manual RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b346b-a0f4-4289-9b44-ed17f3f97750",
   "metadata": {},
   "source": [
    "In this first part, you'll build all of the pieces of the RAG system individually.\n",
    "\n",
    "First, you'll need the retriever portion. Create a FAISS index to hold the text of the article. Encode this text using the all-MiniLM-L6-v2 encoder. Note that you'll want to divide the text into smaller chunks rather than encoding the whole artile all at once. You could try, for example, the [RecursiveCharacterTextSplitter class from LangChain](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html). You'll need to specify a chunk_size and chunk_overlap. You could try a chunk_size of 500 and overlap of 50 as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed1426da-f103-4d14-bc45-72edcb088573",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/2505.23724v1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    article = f.read()\n",
    "    #article = article.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4252f517-344d-4d12-9075-f27a63e379ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc0a8cc7-a69f-4f21-9414-e68a0fe1515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_article = text_splitter.split_text(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa25954c-2fc4-407c-ac46-b9094336d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e99ebc7e-47ab-47ae-ac53-06b06d8e443c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19c456b9d004388a6bcb8c9264b422d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles_vector = embedder.encode(chunked_article, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d67643b-f6ed-44dd-a831-35025ee57ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = articles_vector.shape[1]\n",
    "\n",
    "faiss_index = faiss.IndexFlatIP(d)   # build the index\n",
    "faiss_index.add(articles_vector)       # add vectors to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89524461-d1db-4b65-92b8-1dfb50c58b4c",
   "metadata": {},
   "source": [
    "Next, you'll need to set up a way to interact with the generator model. You can use the OpenAI class from the openai library for this. See [this page](https://platform.openai.com/docs/api-reference/chat/create) for more information. When you do this, you'll need to set the base_url to [\"https://openrouter.ai/api/v1\"](https://openrouter.ai/api/v1) and to pass in your api key. Set the model to \"meta-llama/llama-4-scout:free\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "116d80ee-57c0-4dfc-93d3-57793c14632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model_name=\"meta-llama/llama-4-scout:free\",\n",
    "    openai_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743fb96c-7f00-4a23-9825-9ae0a698e9da",
   "metadata": {},
   "source": [
    "First, ask the model \"How does SC-LoRA differ from regular LoRA?\" without providing any additional context. Read through a few different responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6154ba6-a832-4181-ab96-e60b9a5a7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does SC-LoRA differ from regular LoRA?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "af2bded1-df9b-489c-8367-83ff3fd3e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0e79a95f-63e2-4e85-9645-9afc42c37ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC-LoRA (Space-Conditioned Low-Rank Adaptation) is an adaptation of LoRA (Low-Rank Adaptation), which is a method used in transformer-based models for efficient fine-tuning. The primary distinction between SC-LoRA and regular LoRA lies in how they condition or adapt the model's parameters during the fine-tuning process.\n",
      "\n",
      "1. **Regular LoRA**: LoRA introduces a low-rank matrix that is learned during fine-tuning. This matrix is used to adapt the weights of the model. Specifically, for a given weight matrix \\(W\\), LoRA updates it as \\(W + \\Delta W\\), where \\(\\Delta W = BA\\), and \\(B\\) and \\(A\\) are low-rank matrices learned during training. The key here is that the adaptation \\(\\Delta W\\) is not dependent on the input but is a fixed adaptation learned during fine-tuning.\n",
      "\n",
      "2. **SC-LoRA**: SC-LoRA enhances the basic LoRA approach by conditioning the adaptation on the input space or certain conditions. This means that instead of having a static \\(\\Delta W\\), SC-LoRA allows \\(\\Delta W\\) to vary based on the input or specific conditions (like task or context). This is achieved by incorporating a conditioning mechanism that generates the low-rank adaptation matrices \\(B\\) and \\(A\\) based on the input or other given conditions.\n",
      "\n",
      "The main differences are:\n",
      "\n",
      "- **Conditioning on Input or Context**: SC-LoRA conditions the low-rank adaptation on the input or specific context, making the adaptation more flexible and potentially more effective for a wider range of inputs or tasks.\n",
      "\n",
      "- **Dynamic vs. Static Adaptation**: While regular LoRA learns a static adaptation during fine-tuning, SC-LoRA generates a dynamic adaptation that can change with different inputs or conditions.\n",
      "\n",
      "- **Increased Flexibility and Expressiveness**: By allowing the adaptation to depend on the input or context, SC-LoRA can potentially offer more flexibility and expressiveness than regular LoRA, which might be beneficial for complex tasks or for tasks where the input data distribution is wide or dynamic.\n",
      "\n",
      "Overall, SC-LoRA appears to be an advancement over regular LoRA by offering a more dynamic and input-dependent adaptation mechanism, which could be particularly useful in scenarios where the model needs to adapt to varying conditions or inputs.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e1a902f4-934b-4863-ad3c-8f9b28e4ffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC-LoRA (Structured Compression-LoRA) and LoRA (Low-Rank Adaptation) are both efficient fine-tuning methods for large language models, but they differ in their approach and structure:\n",
      "\n",
      "**LoRA (Low-Rank Adaptation)**\n",
      "\n",
      "LoRA is a method that adapts a pre-trained model to a specific task by adding low-rank matrices to the model's weights. The key idea is to update only a small subset of the model's parameters, specifically by adding a low-rank matrix to the weight matrix of a given layer. This allows for efficient adaptation to new tasks with a relatively small number of additional parameters.\n",
      "\n",
      "**SC-LoRA (Structured Compression-LoRA)**\n",
      "\n",
      "SC-LoRA builds upon LoRA by introducing an additional structured compression component. The main differences between SC-LoRA and LoRA are:\n",
      "\n",
      "1. **Structured pruning**: SC-LoRA incorporates structured pruning, which eliminates entire groups of parameters or neurons, leading to a more compact and efficient model. This pruning is done in a way that maintains the model's performance while reducing the number of parameters.\n",
      "2. **Compression-aware adaptation**: SC-LoRA takes into account the compression of the model during the adaptation process. This means that the method optimizes the low-rank matrices while considering the pruning of parameters, resulting in a more efficient and compact model.\n",
      "\n",
      "**Key benefits of SC-LoRA over LoRA**\n",
      "\n",
      "The advantages of SC-LoRA over LoRA include:\n",
      "\n",
      "* **Improved efficiency**: SC-LoRA leads to more compact models, as the structured pruning reduces the number of parameters.\n",
      "* **Better trade-off between performance and efficiency**: SC-LoRA can achieve a better balance between task performance and model size.\n",
      "\n",
      "Overall, SC-LoRA enhances the LoRA method by incorporating structured compression, which enables more efficient and compact models while maintaining performance.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274ba94-a734-4840-b16b-5945fc6de4d6",
   "metadata": {},
   "source": [
    "Next, use the following as a system prompt:\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    f\"Context: {context}\"\n",
    ")\n",
    "Use the FAISS index to pull in relevant context to fill in the context. Try passing in this additional system prompt. Hint: you can do this by using the following messages in the client.chat.completions.create function\n",
    "\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "    ]\n",
    "How does adding this context change the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "709c5dab-fb09-4caa-bae1-93e9f5ba9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = embedder.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4b25914d-2b9b-4f7f-96d8-5026fbbc06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "distances, indices = faiss_index.search(np.array([query_vector], dtype=np.float32), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8b24c967-3967-405b-b8d2-4b5e1fdbb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = chunked_article[np.take(indices, indices = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6b45dcff-5938-41af-8627-5890373da51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    f\"Context: {context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7345a5e3-2c2d-4fe1-a84e-c9b7b286db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "12114a20-690d-4ff6-a69f-474c4317f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a2a1e7a0-ecd2-49a1-a129-e5d8356b460b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC-LoRA has a β value of 0.9, which is not specified for regular LoRA. SC-LoRA also outperforms LoRA in terms of utility and safety metrics. LoRA's performance varies with learning rate, whereas SC-LoRA achieves a balance between utility and safety.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8c3c7f5d-0f99-4904-96c3-e7f6e3a9e4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC-LoRA has a β value of 0.9, which seems to make it outperform regular LoRA in terms of utility and safety. Regular LoRA shows a decline in safety alignment when the learning rate is increased, but SC-LoRA doesn't exhibit this degradation. The exact differences between SC-LoRA and LoRA are not specified, but β=0.9 appears to be a key factor.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8984d381-14a2-4865-aa81-79ba5bcdf7bf",
   "metadata": {},
   "source": [
    "### Part 2: LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fcf2c3-c4b1-4018-8f29-4ec4f8e19d8b",
   "metadata": {},
   "source": [
    "You can also use the [LangChain library](https://www.langchain.com/) to help build your RAG system.\n",
    "\n",
    "For the retriever, you can use the [HugginFaceEmbeddings class](https://python.langchain.com/api_reference/huggingface/embeddings/langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings.html), using the all-MiniLM-L6-v2 model, to create your embedding model. There is also a [FAISS class](https://python.langchain.com/docs/integrations/vectorstores/faiss/), which has a useful [from_texts method](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS.from_texts). Once you've created your vector store, use the [as_retriever method](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS.as_retriever) on it and save it to a variable named retriever.\n",
    "\n",
    "For the generator, you can use the [ChatOpenAI class](https://python.langchain.com/docs/integrations/chat/openai/). Be sure to set base_url=\"[https://openrouter.ai/api/v1](https://openrouter.ai/api/v1)\", model_name=\"meta-llama/llama-4-scout:free\", and openai_api_key= Your API key. Save this to a variable named llm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cbf967-5b31-412d-a720-4a1bed3497bf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now that the two components have been created, we can combine them into a chat template using the ChatPromptTemplate class. We can set up a system prompt and the pass that in, like\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "Then, you can use the create_stuff_documents_chain function, passing in your llm and the prompt, and then create a chain using the create_retrieval_chain function, passing in the retriever and the chain you just created.\n",
    "\n",
    "Finally, you can use the invoke method to pass in your query as input. See the example on this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3c9b1-f8f9-43a6-be79-5773e012f8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
