{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b19d95-3069-434a-bcb0-3166918bf8c6",
   "metadata": {},
   "source": [
    "##### Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5188fba7-484c-4457-8d47-fba2629bea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cd0862e6-6b9e-4abb-863d-0b130746f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../open_router_api_key.txt\", \"r\") as fi:\n",
    "    api_key = fi.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "102334e4-a16f-4e54-897a-05f8d893820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "#os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce45c5-f84e-48b5-86ef-0fe60d5d61ea",
   "metadata": {},
   "source": [
    "In this exercise, you'll put together a RAG system and compare outputs from RAG vs. just querying an LLM.\n",
    "\n",
    "For this exercise, you'll be asking about Subspace-Constrained LoRA (SC-LoRA), a new technique described in [a recent article publised on arXiv.org](https://arxiv.org/abs/2505.23724). You've been provided the text of this article in the file 2505.23724v1.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b2740-5dc3-4808-a24e-5610525ee7bb",
   "metadata": {},
   "source": [
    "### Part 1: Manual RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b346b-a0f4-4289-9b44-ed17f3f97750",
   "metadata": {},
   "source": [
    "In this first part, you'll build all of the pieces of the RAG system individually.\n",
    "\n",
    "First, you'll need the retriever portion. Create a FAISS index to hold the text of the article. Encode this text using the all-MiniLM-L6-v2 encoder. Note that you'll want to divide the text into smaller chunks rather than encoding the whole artile all at once. You could try, for example, the [RecursiveCharacterTextSplitter class from LangChain](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html). You'll need to specify a chunk_size and chunk_overlap. You could try a chunk_size of 500 and overlap of 50 as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed1426da-f103-4d14-bc45-72edcb088573",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/2505.23724v1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    article = f.read()\n",
    "    #article = article.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4252f517-344d-4d12-9075-f27a63e379ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc0a8cc7-a69f-4f21-9414-e68a0fe1515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_article = text_splitter.split_text(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa25954c-2fc4-407c-ac46-b9094336d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e99ebc7e-47ab-47ae-ac53-06b06d8e443c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19c456b9d004388a6bcb8c9264b422d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles_vector = embedder.encode(chunked_article, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d67643b-f6ed-44dd-a831-35025ee57ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = articles_vector.shape[1]\n",
    "\n",
    "faiss_index = faiss.IndexFlatIP(d)   # build the index\n",
    "faiss_index.add(articles_vector)       # add vectors to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89524461-d1db-4b65-92b8-1dfb50c58b4c",
   "metadata": {},
   "source": [
    "Next, you'll need to set up a way to interact with the generator model. You can use the OpenAI class from the openai library for this. See [this page](https://platform.openai.com/docs/api-reference/chat/create) for more information. When you do this, you'll need to set the base_url to [\"https://openrouter.ai/api/v1\"](https://openrouter.ai/api/v1) and to pass in your api key. Set the model to \"meta-llama/llama-4-scout:free\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "116d80ee-57c0-4dfc-93d3-57793c14632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = api_key,\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743fb96c-7f00-4a23-9825-9ae0a698e9da",
   "metadata": {},
   "source": [
    "First, ask the model \"How does SC-LoRA differ from regular LoRA?\" without providing any additional context. Read through a few different responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6154ba6-a832-4181-ab96-e60b9a5a7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does SC-LoRA differ from regular LoRA?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "99b052b5-2d2d-4e39-9265-a17fb58d7f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"SC-LoRA stands for Scale-aware Low-Rank Adaptation, which is an extension or a variation of the Low-Rank Adaptation (LoRA) method. LoRA is a technique commonly used in the context of large language models and other neural networks to efficiently adapt or fine-tune these models for specific tasks or datasets without requiring full model updates. This is achieved by adding low-rank matrices to the original model's weights.\\n\\nThe main differences between SC-LoRA and regular LoRA are:\\n\\n1. **Scale Awareness**: SC-LoRA introduces scale-awareness into the adaptation process. This means SC-LoRA takes into account the scale or magnitude of the updates or the parameters being adapted. This can be particularly useful in scenarios where the scale of the model's parameters or the adaptation updates significantly impacts the performance or the convergence of the model.\\n\\n2. **Parameter Efficiency and Flexibility**: SC-LoRA aims to offer more flexible and potentially more efficient adaptation by considering the scale of the low-rank updates. This can allow for more nuanced adaptations that better suit the target task or dataset.\\n\\n3. **Robustness and Generalization**: By incorporating scale-awareness, SC-LoRA may also enhance the robustness and generalization capabilities of the adapted model. This is because the method can adapt more finely to the data, potentially reducing overfitting and improving performance on unseen data.\\n\\n4. **Implementation and Computational Cost**: The specific implementation details and computational cost of SC-LoRA compared to LoRA can vary. However, SC-LoRA is designed to be efficient and practical for large-scale models and datasets, similar to LoRA.\\n\\nIn summary, SC-LoRA differs from regular LoRA by incorporating scale-awareness into the low-rank adaptation process, which can lead to more flexible, efficient, and robust adaptations of large models for specific tasks or datasets.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"meta-llama/llama-4-scout:free\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e1a902f4-934b-4863-ad3c-8f9b28e4ffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='SC-LoRA stands for Scale-Customizable Low-Rank Adaptation, which is an extension or a variation of the Low-Rank Adaptation (LoRA) method. LoRA is a technique used in machine learning, particularly in the context of large language models and other neural networks, to adapt or fine-tune these models efficiently for specific tasks or datasets. It achieves this by updating a small subset of the model\\'s parameters, specifically through low-rank matrices, rather than updating the entire model. This approach significantly reduces the number of trainable parameters, making the adaptation process more efficient in terms of computational resources and memory usage.\\n\\nSC-LoRA differs from regular LoRA primarily in its approach to scaling:\\n\\n1. **Scaling Factor**: SC-LoRA introduces a scaling factor that allows for more customization in the adaptation process. This scaling factor can adjust the influence of the adapted low-rank matrices on the original model\\'s weights. In contrast, regular LoRA applies a fixed adaptation without an additional scaling mechanism.\\n\\n2. **Customizability**: The \"scale-customizable\" aspect of SC-LoRA implies that the method provides more flexibility in how the low-rank adaptations are integrated with the base model. This can be particularly useful for different tasks or datasets where the optimal adaptation might require varying strengths of modification.\\n\\n3. **Efficiency and Effectiveness**: By allowing for a scaling factor, SC-LoRA can potentially offer better performance than regular LoRA on certain tasks. The scaling factor can help in fine-tuning the adaptation to better suit the target task or dataset, making SC-LoRA more effective. Additionally, this can be achieved while maintaining a similar level of efficiency in terms of computational cost.\\n\\nIn summary, SC-LoRA enhances the LoRA method by introducing a scaling factor that allows for more customized adaptations of the model. This can lead to improved performance on specific tasks by adjusting the scale of the low-rank adaptations, setting it apart from the standard LoRA approach.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"meta-llama/llama-4-scout:free\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274ba94-a734-4840-b16b-5945fc6de4d6",
   "metadata": {},
   "source": [
    "Next, use the following as a system prompt:\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    f\"Context: {context}\"\n",
    ")\n",
    "Use the FAISS index to pull in relevant context to fill in the context. Try passing in this additional system prompt. Hint: you can do this by using the following messages in the client.chat.completions.create function\n",
    "\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "    ]\n",
    "How does adding this context change the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "709c5dab-fb09-4caa-bae1-93e9f5ba9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = embedder.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4b25914d-2b9b-4f7f-96d8-5026fbbc06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "distances, indices = faiss_index.search(np.array([query_vector], dtype=np.float32), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0900179a-4b3e-4bce-bf79-c2828b7e5400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'methods, both in utility and safety metric. Com-\\npared to the original model, SC-LoRA ( β= 0.9)\\nexhibits almost no safety degradation, and achieves\\nbest utility, even surpassing full fine-tuning by 3.79\\npoints. When increasing the learning rate, LoRA\\nshows a sharp decline in safety alignment while\\nmath ability is increasing. LoRA (lr=2e-5) and\\nCorDA KPA, though preserving safety well, are\\ninsufficient in fine-tuning performance compared\\nto our method. PiSSA and CorDA IPA, though'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_article[np.take(indices, indices = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8b24c967-3967-405b-b8d2-4b5e1fdbb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = chunked_article[np.take(indices, indices = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6b45dcff-5938-41af-8627-5890373da51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    f\"Context: {context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8c3c7f5d-0f99-4904-96c3-e7f6e3a9e4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='SC-LoRA has a β value of 0.9, which seems to make it outperform regular LoRA in terms of utility and safety. Unlike LoRA, which shows a decline in safety alignment when increasing the learning rate, SC-LoRA exhibits almost no safety degradation. The exact differences between SC-LoRA and LoRA are not specified, but the β value and learning rate may contribute to the performance variations.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout:free\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "            },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "         ]\n",
    "    )\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8984d381-14a2-4865-aa81-79ba5bcdf7bf",
   "metadata": {},
   "source": [
    "### Part 2: LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fcf2c3-c4b1-4018-8f29-4ec4f8e19d8b",
   "metadata": {},
   "source": [
    "You can also use the [LangChain library](https://www.langchain.com/) to help build your RAG system.\n",
    "\n",
    "For the retriever, you can use the [HugginFaceEmbeddings class](https://python.langchain.com/api_reference/huggingface/embeddings/langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings.html), using the all-MiniLM-L6-v2 model, to create your embedding model. There is also a [FAISS class](https://python.langchain.com/docs/integrations/vectorstores/faiss/), which has a useful [from_texts method](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS.from_texts). Once you've created your vector store, use the [as_retriever method](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS.as_retriever) on it and save it to a variable named retriever.\n",
    "\n",
    "For the generator, you can use the [ChatOpenAI class](https://python.langchain.com/docs/integrations/chat/openai/). Be sure to set base_url=\"[https://openrouter.ai/api/v1](https://openrouter.ai/api/v1)\", model_name=\"meta-llama/llama-4-scout:free\", and openai_api_key= Your API key. Save this to a variable named llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "44f3ab89-de8c-40fd-8eeb-03d0a530d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9059fbf5-f4d9-460f-9916-2a1c333e1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faiss = FAISS.from_texts(texts, embeddings)\n",
    "faiss = FAISS.from_texts(chunked_article, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5dcb6a4f-aaa2-42c7-af65-6878ad1587d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6c98c3a9-71c5-4e55-9eaa-e3f188116d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model_name=\"meta-llama/llama-4-scout:free\",\n",
    "    openai_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cbf967-5b31-412d-a720-4a1bed3497bf",
   "metadata": {},
   "source": [
    "Now that the two components have been created, we can combine them into a chat template using the [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) class. We can set up a system prompt and then pass that in, like\n",
    "\n",
    "system_prompt = (  \n",
    "    \"Use the given context to answer the question. \"  \n",
    "    \"If you don't know the answer, say you don't know. \"  \n",
    "    \"Use three sentence maximum and keep the answer concise. \"  \n",
    "    \"Context: {context}\"  \n",
    ")  \n",
    "  \n",
    "prompt = ChatPromptTemplate.from_messages(  \n",
    "    [  \n",
    "        (\"system\", system_prompt),  \n",
    "        (\"human\", \"{input}\"),  \n",
    "    ]  \n",
    ")  \n",
    "  \n",
    "Then, you can use the [create_stuff_documents_chain function](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html), passing in your llm and the prompt, and then create a chain using the [create_retrieval_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html) function, passing in the retriever and the chain you just created.\n",
    "\n",
    "Finally, you can use the invoke method to pass in your query as input. See the example on [this page](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "effe216b-15a6-4dc5-9d7b-74779a3266cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "0d318919-d78c-46c2-b81b-8ebedda08ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c70bbe6d-5ce3-45c7-9f57-4b9647dea993",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "f55dac8b-3a89-489d-b1da-8d5e396bba48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does SC-LoRA differ from regular LoRA?',\n",
       " 'context': [Document(id='870459b3-7640-4535-a499-b250fd2ebe81', metadata={}, page_content='methods, both in utility and safety metric. Com-\\npared to the original model, SC-LoRA ( β= 0.9)\\nexhibits almost no safety degradation, and achieves\\nbest utility, even surpassing full fine-tuning by 3.79\\npoints. When increasing the learning rate, LoRA\\nshows a sharp decline in safety alignment while\\nmath ability is increasing. LoRA (lr=2e-5) and\\nCorDA KPA, though preserving safety well, are\\ninsufficient in fine-tuning performance compared\\nto our method. PiSSA and CorDA IPA, though'),\n",
       "  Document(id='6a4610e5-713b-48f8-bcfb-312d6c3102d3', metadata={}, page_content='sponses (score = 5) as harmfulness rate . Lower\\nvalues for both metrics indicate stronger safety of\\nthe model.\\n5Method #Params HS↓HR(%) ↓Utility ↑\\nLlama-2-7b-Chat - 1.100 1.212 24.13\\nFull fine-tuning 6738M 1.364 5.455 51.41\\nLoRA 320M 1.176 2.424 50.32\\nPiSSA 320M 1.252 4.242 51.87\\nCorDA IPA 320M 1.209 3.333 44.61\\nCorDA KPA 320M 1.106 0.606 50.89\\nSC-LoRAβ= 0.5 320M 1.161 1.818 52.54\\nβ= 0.7 320M 1.148 1.818 52.07\\nβ= 0.9 320M 1.097 0.000 51.67'),\n",
       "  Document(id='c34a41d3-3cc6-49f2-9470-33bffc42e001', metadata={}, page_content='2019) with the following hyper-parameters: batch\\nsize 128, learning rate 2e-5 (except for experiment\\nin Section 4.2, where we tune the learning rate of\\nbaselines for better performance), cosine annealing\\nlearning rate schedule, warm-up ratio 0.03, and no\\nweight decay. The rank of LoRA and its variants\\nare all set to 128 for comparison. For SC-LoRA,\\nwe tune the hyperparameter βto find a good bal-\\nanced result. All experiment results are obtained\\nby running on only one seed.'),\n",
       "  Document(id='bf967a08-cf99-43e7-a2c0-187dd7731cf5', metadata={}, page_content='Although SC-LoRA can successfully handle both\\nefficient fine-tuning and knowledge preservation at\\nthe same time, it still has drawbacks.\\nFirst, SC-LoRA is just a LoRA initialization\\nmethod, and does not strongly constrain the updates\\nduring fine-tuning process. Hence after fine-tuning\\non more complex tasks and with more steps, the\\nknowledge preservation ability can also drop (see\\nthe preservation drop of NQ-open in Table 3 for\\nexample).\\nSecond, its application on preserving other types')],\n",
       " 'answer': 'SC-LoRA is a LoRA initialization method that modifies the base LoRA approach with an additional hyperparameter β to balance safety and utility. This allows SC-LoRA to achieve better safety and utility performance compared to regular LoRA. Specifically, SC-LoRA with β=0.9 exhibits improved safety and utility metrics.'}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea134b0-6120-4b8d-9f04-827e6b12e633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
