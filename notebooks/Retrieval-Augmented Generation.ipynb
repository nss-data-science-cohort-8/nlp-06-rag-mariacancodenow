{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b19d95-3069-434a-bcb0-3166918bf8c6",
   "metadata": {},
   "source": [
    "##### Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5188fba7-484c-4457-8d47-fba2629bea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings#, HuggingFacePipeline\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cd0862e6-6b9e-4abb-863d-0b130746f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../open_router_api_key.txt\", \"r\") as fi:\n",
    "    api_key = fi.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "102334e4-a16f-4e54-897a-05f8d893820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "#os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce45c5-f84e-48b5-86ef-0fe60d5d61ea",
   "metadata": {},
   "source": [
    "In this exercise, you'll put together a RAG system and compare outputs from RAG vs. just querying an LLM.\n",
    "\n",
    "For this exercise, you'll be asking about Subspace-Constrained LoRA (SC-LoRA), a new technique described in [a recent article publised on arXiv.org](https://arxiv.org/abs/2505.23724). You've been provided the text of this article in the file 2505.23724v1.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b2740-5dc3-4808-a24e-5610525ee7bb",
   "metadata": {},
   "source": [
    "### Part 1: Manual RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b346b-a0f4-4289-9b44-ed17f3f97750",
   "metadata": {},
   "source": [
    "In this first part, you'll build all of the pieces of the RAG system individually.\n",
    "\n",
    "First, you'll need the retriever portion. Create a FAISS index to hold the text of the article. Encode this text using the all-MiniLM-L6-v2 encoder. Note that you'll want to divide the text into smaller chunks rather than encoding the whole artile all at once. You could try, for example, the [RecursiveCharacterTextSplitter class from LangChain](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html). You'll need to specify a chunk_size and chunk_overlap. You could try a chunk_size of 500 and overlap of 50 as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed1426da-f103-4d14-bc45-72edcb088573",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/2505.23724v1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    article = f.read()\n",
    "    #article = article.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4252f517-344d-4d12-9075-f27a63e379ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc0a8cc7-a69f-4f21-9414-e68a0fe1515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_article = text_splitter.split_text(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa25954c-2fc4-407c-ac46-b9094336d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e99ebc7e-47ab-47ae-ac53-06b06d8e443c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19c456b9d004388a6bcb8c9264b422d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles_vector = embedder.encode(chunked_article, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d67643b-f6ed-44dd-a831-35025ee57ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = articles_vector.shape[1]\n",
    "\n",
    "faiss_index = faiss.IndexFlatIP(d)   # build the index\n",
    "faiss_index.add(articles_vector)     # add vectors to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89524461-d1db-4b65-92b8-1dfb50c58b4c",
   "metadata": {},
   "source": [
    "Next, you'll need to set up a way to interact with the generator model. You can use the OpenAI class from the openai library for this. See [this page](https://platform.openai.com/docs/api-reference/chat/create) for more information. When you do this, you'll need to set the base_url to [\"https://openrouter.ai/api/v1\"](https://openrouter.ai/api/v1) and to pass in your api key. Set the model to \"meta-llama/llama-4-scout:free\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "116d80ee-57c0-4dfc-93d3-57793c14632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = api_key,\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743fb96c-7f00-4a23-9825-9ae0a698e9da",
   "metadata": {},
   "source": [
    "First, ask the model \"How does SC-LoRA differ from regular LoRA?\" without providing any additional context. Read through a few different responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6154ba6-a832-4181-ab96-e60b9a5a7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does SC-LoRA differ from regular LoRA?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "99b052b5-2d2d-4e39-9265-a17fb58d7f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='SC-LoRA (Structured and Controlled Low-Rank Adaptation) is an extension or a variation of LoRA (Low-Rank Adaptation), which is a method used in the context of large language models and other neural networks to adapt or fine-tune these models efficiently. While both SC-LoRA and LoRA aim to achieve efficient adaptation of large models with a minimal number of additional parameters, they differ in their approach and objectives:\\n\\n1. **LoRA (Low-Rank Adaptation):** \\n   - LoRA is designed to adapt large pre-trained models to specific tasks with a relatively small number of additional parameters. It achieves this by introducing low-rank matrices that are learned during the adaptation process. These low-rank matrices are used to update the weights of the original model in a way that is efficient in terms of the number of parameters and computations required.\\n   - The primary goal of LoRA is to reduce the number of trainable parameters during adaptation, making it more computationally efficient and reducing the risk of overfitting.\\n\\n2. **SC-LoRA (Structured and Controlled Low-Rank Adaptation):**\\n   - SC-LoRA builds upon the LoRA framework but introduces additional constraints and structures to the adaptation process. The \"Structured\" part refers to imposing certain structural constraints on the low-rank matrices learned during adaptation, which could help in preserving certain properties of the original model or ensuring that the adapted model has a specific form that is beneficial for the target task.\\n   - The \"Controlled\" aspect suggests that SC-LoRA allows for more explicit control over the adaptation process. This could involve mechanisms to regulate how much the original model\\'s weights are changed, ensuring that the adapted model does not deviate too far from its original form or that it maintains certain desirable properties.\\n\\n**Key Differences:**\\n- **Structural Constraints:** SC-LoRA introduces additional structural constraints on the adaptation process, aiming to preserve certain properties or achieve specific objectives that are beneficial for the target task.\\n- **Control Mechanisms:** SC-LoRA provides more explicit control over how the model is adapted, which can help in managing the trade-off between adaptation to the new task and preservation of the original model\\'s capabilities.\\n- **Objectives:** While LoRA focuses on efficient adaptation with minimal parameters, SC-LoRA seems to balance efficiency with additional objectives such as maintaining model interpretability, stability, or specific structural properties.\\n\\nOverall, SC-LoRA can be seen as an enhanced version of LoRA that not only aims for efficient adaptation but also considers structural and control aspects to potentially improve the adaptability, stability, and applicability of the adapted models across different tasks.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"meta-llama/llama-4-scout:free\",\n",
    "  messages=[\n",
    "    #{\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e1a902f4-934b-4863-ad3c-8f9b28e4ffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"SC-LoRA, or Scalable Low-Rank Adaptation, and LoRA (Low-Rank Adaptation) are both methods used for efficient fine-tuning of large pre-trained models, such as those used in natural language processing and computer vision. While they share some similarities, SC-LoRA is an advancement over the traditional LoRA method, primarily focusing on improving scalability and efficiency. Here's how SC-LoRA differs from regular LoRA:\\n\\n1. **Scalability and Efficiency:**\\n   - **LoRA:** LoRA is designed to adapt large pre-trained models to specific tasks more efficiently than full fine-tuning. It achieves this by updating only a small portion of the model's parameters, specifically through low-rank matrices that are learned during the fine-tuning process. While LoRA is efficient for a single task adaptation, managing and deploying multiple task-specific models can become cumbersome and less scalable as the number of tasks increases.\\n   - **SC-LoRA:** SC-LoRA builds upon LoRA's efficiency and scalability by further enhancing how task-specific adaptations are managed and deployed. SC-LoRA introduces a more scalable approach, potentially allowing for a more unified and manageable way to adapt models across multiple tasks without needing to store and deploy separate models for each task.\\n\\n2. **Parameter Sharing and Task Interference:**\\n   - **LoRA:** In LoRA, each task has its own set of learned low-rank matrices, which are used to adapt the pre-trained model. This can lead to a scenario where the storage and deployment requirements grow linearly with the number of tasks.\\n   - **SC-LoRA:** SC-LoRA might aim to mitigate the issue of growing storage requirements and potential task interference by promoting parameter sharing across tasks or introducing mechanisms that allow for more efficient multi-task adaptation. This could help in scenarios where tasks are related or when computational resources are limited.\\n\\n3. **Adaptation Mechanism:**\\n   - **LoRA:** LoRA adapts the model by adding low-rank matrices (learned during training) to specific layers of the pre-trained model. This adaptation is task-specific but does not inherently leverage relationships or similarities between tasks.\\n   - **SC-LoRA:** The exact adaptation mechanism of SC-LoRA could vary, but it is designed to enhance scalability. This might involve more sophisticated sharing of learned adaptations, task clustering, or other strategies that enable efficient adaptation across a large number of tasks.\\n\\n4. **Multi-Task Learning Perspective:**\\n   - **LoRA:** Focuses on single-task adaptation primarily.\\n   - **SC-LoRA:** May incorporate principles from multi-task learning, aiming to find a more unified model adaptation that can efficiently handle multiple tasks, possibly by learning a meta-adaptation strategy that can be applied across tasks.\\n\\nIn summary, while LoRA is an efficient method for adapting large pre-trained models to specific tasks, SC-LoRA aims to push the boundaries further by focusing on scalability, potentially through better parameter management, task adaptation sharing, and multi-task learning strategies. The precise differences would depend on the specific design and goals of the SC-LoRA method.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"meta-llama/llama-4-scout:free\",\n",
    "  messages=[\n",
    "    #{\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274ba94-a734-4840-b16b-5945fc6de4d6",
   "metadata": {},
   "source": [
    "Next, use the following as a system prompt:\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    f\"Context: {context}\"\n",
    ")\n",
    "Use the FAISS index to pull in relevant context to fill in the context. Try passing in this additional system prompt. Hint: you can do this by using the following messages in the client.chat.completions.create function\n",
    "\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "    ]\n",
    "How does adding this context change the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "709c5dab-fb09-4caa-bae1-93e9f5ba9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = embedder.encode([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "4d79174f-4e22-419f-91de-d7ec0423c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "distances, indices = faiss_index.search(query_vector, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2fbb76a9-0fbd-4a05-b6b5-00c864059c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated with Michael's code to add chunks (I had issues with query_vector formatting...)\n",
    "most_similar_chunks = indices[0]\n",
    "context = ''\n",
    "for i in most_similar_chunks:\n",
    "    context += '\\n\\n' + chunked_article[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac4568-33aa-40d8-b435-b2589c66b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6b45dcff-5938-41af-8627-5890373da51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    f\"Context: {context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8c3c7f5d-0f99-4904-96c3-e7f6e3a9e4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC-LoRA is a LoRA initialization method that modifies the beta parameter (β) to balance utility and safety, whereas regular LoRA has a fixed learning rate and does not have this beta parameter. SC-LoRA aims to preserve safety and knowledge while fine-tuning, and its β values (e.g., 0.5, 0.7, 0.9) control this balance. This allows SC-LoRA to achieve better safety and utility performance than regular LoRA.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout:free\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "            },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "         ]\n",
    "    )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8984d381-14a2-4865-aa81-79ba5bcdf7bf",
   "metadata": {},
   "source": [
    "### Part 2: LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fcf2c3-c4b1-4018-8f29-4ec4f8e19d8b",
   "metadata": {},
   "source": [
    "You can also use the [LangChain library](https://www.langchain.com/) to help build your RAG system.\n",
    "\n",
    "For the retriever, you can use the [HugginFaceEmbeddings class](https://python.langchain.com/api_reference/huggingface/embeddings/langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings.html), using the all-MiniLM-L6-v2 model, to create your embedding model. There is also a [FAISS class](https://python.langchain.com/docs/integrations/vectorstores/faiss/), which has a useful [from_texts method](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS.from_texts). Once you've created your vector store, use the [as_retriever method](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS.as_retriever) on it and save it to a variable named retriever.\n",
    "\n",
    "For the generator, you can use the [ChatOpenAI class](https://python.langchain.com/docs/integrations/chat/openai/). Be sure to set base_url=\"[https://openrouter.ai/api/v1](https://openrouter.ai/api/v1)\", model_name=\"meta-llama/llama-4-scout:free\", and openai_api_key= Your API key. Save this to a variable named llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "44f3ab89-de8c-40fd-8eeb-03d0a530d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9059fbf5-f4d9-460f-9916-2a1c333e1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faiss = FAISS.from_texts(texts, embeddings)\n",
    "faiss = FAISS.from_texts(chunked_article, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5dcb6a4f-aaa2-42c7-af65-6878ad1587d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6c98c3a9-71c5-4e55-9eaa-e3f188116d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model_name=\"meta-llama/llama-4-scout:free\",\n",
    "    openai_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cbf967-5b31-412d-a720-4a1bed3497bf",
   "metadata": {},
   "source": [
    "Now that the two components have been created, we can combine them into a chat template using the [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) class. We can set up a system prompt and then pass that in, like\n",
    "\n",
    "system_prompt = (  \n",
    "    \"Use the given context to answer the question. \"  \n",
    "    \"If you don't know the answer, say you don't know. \"  \n",
    "    \"Use three sentence maximum and keep the answer concise. \"  \n",
    "    \"Context: {context}\"  \n",
    ")  \n",
    "  \n",
    "prompt = ChatPromptTemplate.from_messages(  \n",
    "    [  \n",
    "        (\"system\", system_prompt),  \n",
    "        (\"human\", \"{input}\"),  \n",
    "    ]  \n",
    ")  \n",
    "  \n",
    "Then, you can use the [create_stuff_documents_chain function](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html), passing in your llm and the prompt, and then create a chain using the [create_retrieval_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html) function, passing in the retriever and the chain you just created.\n",
    "\n",
    "Finally, you can use the invoke method to pass in your query as input. See the example on [this page](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "effe216b-15a6-4dc5-9d7b-74779a3266cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "0d318919-d78c-46c2-b81b-8ebedda08ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c70bbe6d-5ce3-45c7-9f57-4b9647dea993",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "f55dac8b-3a89-489d-b1da-8d5e396bba48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does SC-LoRA differ from regular LoRA?',\n",
       " 'context': [Document(id='870459b3-7640-4535-a499-b250fd2ebe81', metadata={}, page_content='methods, both in utility and safety metric. Com-\\npared to the original model, SC-LoRA ( β= 0.9)\\nexhibits almost no safety degradation, and achieves\\nbest utility, even surpassing full fine-tuning by 3.79\\npoints. When increasing the learning rate, LoRA\\nshows a sharp decline in safety alignment while\\nmath ability is increasing. LoRA (lr=2e-5) and\\nCorDA KPA, though preserving safety well, are\\ninsufficient in fine-tuning performance compared\\nto our method. PiSSA and CorDA IPA, though'),\n",
       "  Document(id='6a4610e5-713b-48f8-bcfb-312d6c3102d3', metadata={}, page_content='sponses (score = 5) as harmfulness rate . Lower\\nvalues for both metrics indicate stronger safety of\\nthe model.\\n5Method #Params HS↓HR(%) ↓Utility ↑\\nLlama-2-7b-Chat - 1.100 1.212 24.13\\nFull fine-tuning 6738M 1.364 5.455 51.41\\nLoRA 320M 1.176 2.424 50.32\\nPiSSA 320M 1.252 4.242 51.87\\nCorDA IPA 320M 1.209 3.333 44.61\\nCorDA KPA 320M 1.106 0.606 50.89\\nSC-LoRAβ= 0.5 320M 1.161 1.818 52.54\\nβ= 0.7 320M 1.148 1.818 52.07\\nβ= 0.9 320M 1.097 0.000 51.67'),\n",
       "  Document(id='c34a41d3-3cc6-49f2-9470-33bffc42e001', metadata={}, page_content='2019) with the following hyper-parameters: batch\\nsize 128, learning rate 2e-5 (except for experiment\\nin Section 4.2, where we tune the learning rate of\\nbaselines for better performance), cosine annealing\\nlearning rate schedule, warm-up ratio 0.03, and no\\nweight decay. The rank of LoRA and its variants\\nare all set to 128 for comparison. For SC-LoRA,\\nwe tune the hyperparameter βto find a good bal-\\nanced result. All experiment results are obtained\\nby running on only one seed.'),\n",
       "  Document(id='bf967a08-cf99-43e7-a2c0-187dd7731cf5', metadata={}, page_content='Although SC-LoRA can successfully handle both\\nefficient fine-tuning and knowledge preservation at\\nthe same time, it still has drawbacks.\\nFirst, SC-LoRA is just a LoRA initialization\\nmethod, and does not strongly constrain the updates\\nduring fine-tuning process. Hence after fine-tuning\\non more complex tasks and with more steps, the\\nknowledge preservation ability can also drop (see\\nthe preservation drop of NQ-open in Table 3 for\\nexample).\\nSecond, its application on preserving other types')],\n",
       " 'answer': 'SC-LoRA is a LoRA initialization method that modifies the base LoRA approach with an additional hyperparameter β to balance safety and utility. This allows SC-LoRA to achieve better safety and utility performance compared to regular LoRA. Specifically, SC-LoRA with β=0.9 exhibits improved safety and utility metrics.'}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea134b0-6120-4b8d-9f04-827e6b12e633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
